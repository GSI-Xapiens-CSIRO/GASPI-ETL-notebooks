{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "#%%writefile requirements.txt\n",
    "#torch==2.5.1\n",
    "#tensorflow==2.18.0\n",
    "#pandas==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 08:25:24.737826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dummy_medical_data.csv -> output/predicted_dummy_medical_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Konfigurasi Model\n",
    "MODEL_CONFIG = {\n",
    "    \"preprocessing\": {\n",
    "        \"padding\": {\"max_length\": 20, \"padding_mode\": \"pre\"},\n",
    "        \"label_encoding\": {\"pii\": 1, \"non-pii\": 0},\n",
    "        \"tokenizer\": {\"flname\": \"pii_tokenizer.json\"},\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"name\": \"model_hyperparams_epoch50_layer5_lr01.pth\",\n",
    "        \"hyperparams\": {\n",
    "            \"vocab_size\": 28229,\n",
    "            \"output_dim\": 1,\n",
    "            \"embedding_dim\": 128,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"num_layers\": 5,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dummy regex untuk mendeteksi data sensitif\n",
    "NIK_REGEX = re.compile(r\"^(1[1-9]|21|[37][1-6]|5[1-3]|6[1-5]|[89][12])\\d{2}\\d{2}([04][1-9]|[1256][0-9]|[37][01])(0[1-9]|1[0-2])\\d{2}\\d{4}$\")\n",
    "MOBILE_PHONE_REGEX = re.compile(r\"^(\\+62|62)?[\\s-]?0?8[1-9]{1}\\d{1}[\\s-]?\\d{4}[\\s-]?\\d{2,5}$\")\n",
    "PLATE_NUMBER_REGEX = re.compile(r\"^[A-Z]{1,2}\\s{0,1}\\d{0,4}\\s{0,1}[A-Z]{0,3}$\")\n",
    "HOME_PHONE_REGEX = re.compile(r\"^(\\+62|62)?[\\s-]?0?([2-7]|9)\\d(\\d)?[\\s-]?[2-9](\\d){6,7}\")\n",
    "\n",
    "class LSTIMPii(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTIMPii, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_out[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "class PredictingPII:\n",
    "    def __init__(self, input_folder, output_folder):\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.hyperparams = MODEL_CONFIG[\"models\"][\"hyperparams\"]\n",
    "        self.model = self._instantiate_model()\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "\n",
    "    def _instantiate_model(self):\n",
    "        model = LSTIMPii(\n",
    "            vocab_size=self.hyperparams[\"vocab_size\"],\n",
    "            embedding_dim=self.hyperparams[\"embedding_dim\"],\n",
    "            hidden_dim=self.hyperparams[\"hidden_dim\"],\n",
    "            output_dim=self.hyperparams[\"output_dim\"],\n",
    "            num_layers=self.hyperparams[\"num_layers\"],\n",
    "        )\n",
    "        model.load_state_dict(torch.load(MODEL_CONFIG[\"models\"][\"name\"], map_location=self.device))\n",
    "        return model.to(self.device)\n",
    "\n",
    "    def _load_tokenizer(self):\n",
    "        with open(MODEL_CONFIG[\"preprocessing\"][\"tokenizer\"][\"flname\"], \"r\") as file:\n",
    "            return tokenizer_from_json(file.read())\n",
    "\n",
    "    def _preprocess(self, data):\n",
    "        sequences = self.tokenizer.texts_to_sequences(data.astype(str))\n",
    "        padded = pad_sequences(sequences, maxlen=MODEL_CONFIG[\"preprocessing\"][\"padding\"][\"max_length\"], padding=\"pre\")\n",
    "        return torch.tensor(padded, dtype=torch.long).to(self.device)\n",
    "\n",
    "    def predict(self, file_name):\n",
    "        input_path = os.path.join(self.input_folder, file_name)\n",
    "        output_path = os.path.join(self.output_folder, f\"predicted_{file_name}\")\n",
    "\n",
    "        df = pd.read_csv(input_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].astype(str).apply(\n",
    "                lambda x: \"*****\" if NIK_REGEX.match(x) or MOBILE_PHONE_REGEX.match(x) or PLATE_NUMBER_REGEX.match(x) or HOME_PHONE_REGEX.match(x) else x\n",
    "            )\n",
    "            input_data = self._preprocess(df[column])\n",
    "            with torch.no_grad():\n",
    "                predictions = (self.model(input_data).squeeze(1) > 0.5).long()\n",
    "            df[column] = df[column].apply(lambda x: \"*****\" if predictions[0].item() == 1 else x)\n",
    "        \n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {file_name} -> {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = PredictingPII(input_folder=\"input\", output_folder=\"output\")\n",
    "    for file in os.listdir(\"input\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            predictor.predict(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
