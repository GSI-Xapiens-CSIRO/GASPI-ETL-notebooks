{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca5aaa3",
   "metadata": {},
   "source": [
    "# Download and analyse dataportal log events\n",
    "\n",
    "## Downloading log events\n",
    "\n",
    "We are going to analyse the log events of user `admin@example.com` (an admin/manager user).\n",
    "\n",
    "To perform this task, you must have aws console access, because the keys are needed to access aws console via boto3 library.\n",
    "\n",
    "Please ensure keys are added to the jupyter environment before you run the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37961c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"admin@example.com\"\n",
    "region = \"ap-southeast-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690eb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('logs', region_name=region)\n",
    "\n",
    "\n",
    "def iso_to_epoch_millis(iso_str):\n",
    "    dt = datetime.strptime(iso_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    dt = dt.replace(tzinfo=timezone.utc)\n",
    "    epoch_millis = int(dt.timestamp() * 1000)\n",
    "    return epoch_millis\n",
    "\n",
    "def epoch_millis_to_iso(epoch_millis):\n",
    "    dt = datetime.fromtimestamp(epoch_millis / 1000, tz=timezone.utc)\n",
    "    return dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "\n",
    "def get_all_log_events(log_group_name, log_stream_name, region, start_time, end_time):\n",
    "    kwargs = {\n",
    "        'logGroupName': log_group_name,\n",
    "        'logStreamName': log_stream_name,\n",
    "        'startFromHead': True,\n",
    "        'startTime': start_time,\n",
    "        'endTime': end_time\n",
    "    }\n",
    "    \n",
    "    events = []\n",
    "    while True:\n",
    "        response = client.get_log_events(**kwargs)\n",
    "        events.extend(response['events'])\n",
    "        # Pagination: nextForwardToken only changes if there's more data\n",
    "        next_token = response.get('nextForwardToken')\n",
    "        if not next_token or next_token == kwargs.get('nextToken'):\n",
    "            break\n",
    "        kwargs['nextToken'] = next_token\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def get_log_streams(log_group_name, start_time, end_time):\n",
    "    paginator = client.get_paginator('describe_log_streams')\n",
    "    page_iterator = paginator.paginate(\n",
    "        logGroupName=log_group_name,\n",
    "        orderBy='LastEventTime',\n",
    "        descending=True\n",
    "    )\n",
    "    log_streams = []\n",
    "    \n",
    "    for page in page_iterator:\n",
    "        for stream in page['logStreams']:\n",
    "            log_stream_name = stream['logStreamName']\n",
    "            first_event_timestamp = stream.get('firstEventTimestamp')\n",
    "            last_event_timestamp = stream.get('lastEventTimestamp')\n",
    "\n",
    "            if last_event_timestamp is not None and last_event_timestamp < start_time:\n",
    "                break\n",
    "    \n",
    "            if (last_event_timestamp is not None and first_event_timestamp is not None and\n",
    "                last_event_timestamp >= start_time and first_event_timestamp <= end_time):\n",
    "                log_streams.append(log_stream_name)\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    return log_streams\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a405a9f-f117-4de0-a363-c8338eaa0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"dataportal\"\n",
    "log_group_name = \"/aws/lambda/sbeacon-backend-dataPortal\"\n",
    "\n",
    "START_TIME = \"2025-06-03T00:00:00Z\"\n",
    "END_TIME = \"2025-06-05T23:59:59Z\"\n",
    "\n",
    "start_time = iso_to_epoch_millis(START_TIME)\n",
    "end_time = iso_to_epoch_millis(END_TIME)\n",
    "streams = get_log_streams(log_group_name, start_time, end_time)\n",
    "\n",
    "for stream in streams:\n",
    "    # print(f\"Stream - {stream}\")\n",
    "    events = get_all_log_events(log_group_name, stream, region, start_time, end_time)\n",
    "    safe_stream_name = stream.replace(\"/\", \"_\")\n",
    "    with open(f\"{name}_{safe_stream_name}.json\", \"w+\") as fo:\n",
    "        fo.write(json.dumps(events))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db65815",
   "metadata": {},
   "source": [
    "## Loading the events for dataportal log group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540ef555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "def iterate_log_entries():\n",
    "    entries = []\n",
    "    for file in glob(\"dataportal_*.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = f.read()\n",
    "            data = data.replace(\"[]\\n\", \"\")\n",
    "            entries +=  json.loads(data)\n",
    "    \n",
    "    log_entry = []\n",
    "    for entry in entries:\n",
    "        log_entry.append(entry)\n",
    "        if entry[\"message\"].startswith(\"REPORT\"):\n",
    "            yield log_entry\n",
    "            log_entry = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952ed58",
   "metadata": {},
   "source": [
    "Admins can use the sub of this user to track their login and logout activities in cloudtrail. You can get sub of this user using the following command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a83b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws cognito-idp list-users --user-pool-id <user-pool-id> --filter \"email = \\\"<email>\\\"\"\n",
    "\n",
    "# for example\n",
    "# aws cognito-idp list-users --user-pool-id ap-southeast-2_3ZrrcagIG --filter \"email = \\\"admin@example.com\\\"\" --region ap-southeast-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0641aa",
   "metadata": {},
   "source": [
    "Output would look like follows\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Users\": [\n",
    "    {\n",
    "      \"Username\": \"admin@example.com\",\n",
    "      \"Attributes\": [\n",
    "        {\n",
    "          \"Name\": \"email\",\n",
    "          \"Value\": \"admin@example.com\"\n",
    "        },\n",
    "        {\n",
    "          \"Name\": \"email_verified\",\n",
    "          \"Value\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"Name\": \"family_name\",\n",
    "          \"Value\": \"Admin\"\n",
    "        },\n",
    "        {\n",
    "          \"Name\": \"given_name\",\n",
    "          \"Value\": \"Admin\"\n",
    "        },\n",
    "        {\n",
    "          \"Name\": \"custom:terraform\",\n",
    "          \"Value\": \"true\"\n",
    "        },\n",
    "        {\n",
    "          \"Name\": \"custom:identity_id\",\n",
    "          \"Value\": \"ap-southeast-2:099e873d-80b5-cb64-b9b4-0f64c663bd46\"\n",
    "        },\n",
    "        {\n",
    "          \"Name\": \"sub\",\n",
    "          \"Value\": \"f98e24c8-2011-70ae-9d93-084eb3f4b282\"\n",
    "        }\n",
    "      ],\n",
    "      \"UserCreateDate\": \"2024-11-20T15:58:30.157000+10:30\",\n",
    "      \"UserLastModifiedDate\": \"2025-03-13T15:09:11.817000+10:30\",\n",
    "      \"Enabled\": true,\n",
    "      \"UserStatus\": \"CONFIRMED\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ae754",
   "metadata": {},
   "source": [
    "## Dataportal notebook events for the user admin@example.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a851b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User admin@example.com listed notebooks at 2025-06-04T02:27:47Z\n"
     ]
    }
   ],
   "source": [
    "from textwrap import indent\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "re_notebook_start = re.compile(r\"^/dportal/notebooks/.*?/start$\")\n",
    "re_notebook_stop = re.compile(r\"^/dportal/notebooks/.*?/stop$\")\n",
    "re_notebook = re.compile(r\"^/dportal/notebooks/[a-zA-Z0-9-]+$\")\n",
    "\n",
    "for log_entry in iterate_log_entries():\n",
    "    log_event = list(filter(lambda x: x[\"message\"].startswith(\"Event Received\"), log_entry))[0]\n",
    "    event = log_event[\"message\"]\n",
    "    event = event.replace(\"Event Received: \", \"\")\n",
    "    event = json.loads(event)\n",
    "    \n",
    "\n",
    "    if not event[\"requestContext\"][\"authorizer\"][\"claims\"][\"email\"] == user:\n",
    "        continue\n",
    "\n",
    "    if event[\"httpMethod\"] == \"POST\" and event[\"path\"] == \"/dportal/notebooks\":\n",
    "        print(f\"User {user} created a notebook at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "        print(\"\\tNotebook properties:\")\n",
    "        print(indent(json.dumps(json.loads(event[\"body\"]), indent=4), \"\\t\"))\n",
    "\n",
    "    elif re_notebook_start.match(event[\"path\"]):\n",
    "        print(f\"User {user} started notebook: {event['path'].split('/')[-2]}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "    \n",
    "    elif re_notebook_stop.match(event[\"path\"]):\n",
    "        print(f\"User {user} stopped notebook: {event['path'].split('/')[-2]}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    elif re_notebook.match(event[\"path\"]):\n",
    "        print(f\"User {user} listed details of notebook: {event['path'].split('/')[-1]}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    elif \"/dportal/notebooks\" == event[\"path\"]:\n",
    "        print(f\"User {user} listed notebooks at {epoch_millis_to_iso(log_event['timestamp'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb2a6f",
   "metadata": {},
   "source": [
    "## Dataportal manager tasks performed by user admin@example.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9134e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import indent\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "re_projects = re.compile(r\"^/dportal/admin/projects$\")\n",
    "re_project = re.compile(r\"^/dportal/admin/projects/[a-zA-Z%0-9]+$\")\n",
    "re_projects_ingest = re.compile(r\"^/dportal/admin/projects/[a-zA-Z%0-9]+/ingest/[a-zA-Z%0-9-]+$\")\n",
    "re_notebook_delete = re.compile(r\"^/dportal/admin/notebooks/[a-zA-Z-0-9]+/delete$\")\n",
    "re_notebook = re.compile(r\"^/dportal/admin/notebooks/[a-zA-Z-0-9]+$\")\n",
    "\n",
    "for log_entry in iterate_log_entries():\n",
    "    log_event = list(filter(lambda x: x[\"message\"].startswith(\"Event Received\"), log_entry))[0]\n",
    "    event = log_event[\"message\"]\n",
    "    event = event.replace(\"Event Received: \", \"\")\n",
    "    event = json.loads(event)\n",
    "    \n",
    "\n",
    "\n",
    "    if not event[\"requestContext\"][\"authorizer\"][\"claims\"][\"email\"] == user or \"/dportal/admin\" not in event[\"path\"]:\n",
    "        continue\n",
    "\n",
    "    #\n",
    "    # Projects\n",
    "    #\n",
    "    \n",
    "    if event[\"httpMethod\"] == \"POST\" and re_projects.match(event[\"path\"]):\n",
    "        print(f\"User {user} created a project at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "        print(\"\\tProject properties:\")\n",
    "        print(indent(json.dumps(json.loads(event[\"body\"]), indent=4), \"\\t\"))\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"GET\" and re_projects.match(event[\"path\"]):\n",
    "        print(f\"User {user} listed projects at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"GET\" and re_project.match(event[\"path\"]):\n",
    "        print(f\"User {user} listed details of project: {event['path'].split('/')[-1]}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"PUT\" and re_project.match(event[\"path\"]):\n",
    "        print(f\"User {user} updated details of project: {unquote(event['path'].split('/')[-1])}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "        print(\"\\tProject properties:\")\n",
    "        print(indent(json.dumps(json.loads(event[\"body\"]), indent=4), \"\\t\"))\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"POST\" and re_projects_ingest.match(event[\"path\"]):\n",
    "        print(f\"User {user} ingested data into project: {unquote(event['path'].split('/')[-3])}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "        print(\"\\tIngest properties:\")\n",
    "        print(indent(json.dumps(json.loads(event[\"body\"]), indent=4), \"\\t\"))\n",
    "\n",
    "    # \n",
    "    # sBeacon \n",
    "    #\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"POST\" and event[\"path\"] == \"/dportal/admin/sbeacon/index\":\n",
    "        print(f\"User {user} indexed data into sBeacon at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    # \n",
    "    # notebooks\n",
    "    # \n",
    "\n",
    "    elif event[\"httpMethod\"] == \"GET\" and event[\"path\"] == \"/dportal/admin/notebooks\":\n",
    "        print(f\"User {user} listed notebooks at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"GET\" and re_notebook.match(event[\"path\"]):\n",
    "        print(f\"User {user} listed details of notebook: {unquote(event['path'].split('/')[-1])}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"POST\" and re_notebook_delete.match(event[\"path\"]):\n",
    "        print(f\"User {user} deleted notebook: {unquote(event['path'].split('/')[-1])}, at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    elif event[\"httpMethod\"] == \"GET\" and event[\"path\"] == \"/dportal/admin/folders\":\n",
    "        print(f\"User {user} listed folders at {epoch_millis_to_iso(log_event['timestamp'])}\")\n",
    "\n",
    "    else:\n",
    "        print(\"MISSED EVENT\", event[\"httpMethod\"], event[\"path\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c98450",
   "metadata": {},
   "source": [
    "## Dataportal file delete events for the user admin@example.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71799250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import indent\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "re_admin_projects = re.compile(r\"^/dportal/admin/projects/[a-zA-Z0-9%-]+$\")\n",
    "\n",
    "\n",
    "for complete_log_entry in iterate_log_entries():\n",
    "    log_event = list(filter(lambda x: x[\"message\"].startswith(\"Event Received\"), complete_log_entry))[0]\n",
    "    event = log_event[\"message\"]\n",
    "    event = event.replace(\"Event Received: \", \"\")\n",
    "    event = json.loads(event)\n",
    "    \n",
    "\n",
    "    if not event[\"requestContext\"][\"authorizer\"][\"claims\"][\"email\"] == user:\n",
    "        continue\n",
    "\n",
    "    if event[\"httpMethod\"] == \"PUT\" and re_admin_projects.match(event[\"path\"]):\n",
    "        print(f\"User {user} updated project: {event['path'].split('/')[-2]}, at {log_event['timestamp']}\")\n",
    "        delete_log_event = list(filter(lambda x: x[\"message\"].startswith(\"Deleting\"), complete_log_entry))\n",
    "\n",
    "        print(\"\\tUpdate payload:\")\n",
    "        print(indent(json.dumps(json.loads(event[\"body\"]), indent=4), \"\\t\"))\n",
    "        for delete_event in delete_log_event:\n",
    "            print(f\"\\tAction: {delete_event['message'].strip()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16b35f",
   "metadata": {},
   "source": [
    "## Dataportal file add events for the user admin@example.com\n",
    "\n",
    "This tracks all file uploads regardless if they are invalid files or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61975a0-9b0b-45e0-83e1-669dc8c95b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_group_name = \"/aws/lambda/sbeacon-backend-deidentifyFiles\"\n",
    "name = \"deidentify\"\n",
    "\n",
    "START_TIME = \"2025-05-01T00:00:00Z\"\n",
    "END_TIME = \"2025-05-30T23:59:59Z\"\n",
    "\n",
    "start_time = iso_to_epoch_millis(START_TIME)\n",
    "end_time = iso_to_epoch_millis(END_TIME)\n",
    "streams = get_log_streams(log_group_name, start_time, end_time)\n",
    "\n",
    "for stream in streams:\n",
    "    # print(f\"Stream - {stream}\")\n",
    "    events = get_all_log_events(log_group_name, stream, region, start_time, end_time)\n",
    "    safe_stream_name = stream.replace(\"/\", \"_\")\n",
    "    with open(f\"{name}_{safe_stream_name}.json\", \"w+\") as fo:\n",
    "        fo.write(json.dumps(events))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe2939",
   "metadata": {},
   "source": [
    "## Loading the events for deidentify log group\n",
    "\n",
    "This is the log group that records file uploads immediately after uploads are completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c5de580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "def iterate_log_entries():\n",
    "    entries = []\n",
    "    for file in glob(\"deidentify_*.json\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            data = f.read()\n",
    "            data = data.replace(\"[]\\n\", \"\")\n",
    "            entries +=  json.loads(data)\n",
    "    \n",
    "    log_entry = []\n",
    "    for entry in entries:\n",
    "        log_entry.append(entry)\n",
    "        if entry[\"message\"].startswith(\"REPORT\"):\n",
    "            yield log_entry\n",
    "            log_entry = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec22290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: \"admin@example.com\" created file: \"wrong.vcf.gz\" in project: \"My test project\" at 1747871597084\n"
     ]
    }
   ],
   "source": [
    "from textwrap import indent\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "for complete_log_entry in iterate_log_entries():\n",
    "    log_event = list(filter(lambda x: x[\"message\"].startswith(\"Backend Event Received:\"), complete_log_entry))[0]\n",
    "\n",
    "    if file_event := list(filter(lambda x: x[\"message\"].startswith(\"File owner\"), complete_log_entry)):\n",
    "        pattern = r'File owner for \"(.*?)\" of project \"(.*?)\" is \"(.*?)\"'\n",
    "        match = re.match(pattern, file_event[0][\"message\"])\n",
    "        if match:\n",
    "            file_name, project, user_sub = match.groups()\n",
    "            print(f'User: \"{user_sub}\" created file: \"{file_name}\" in project: \"{project}\" at {log_event[\"timestamp\"]}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
